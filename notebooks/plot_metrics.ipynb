{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ebf0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa9d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../reports/figures\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4b56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar métricas\n",
    "with open(\"../reports/all_metrics.json\", \"r\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "# convertir a DataFrame\n",
    "rows = []\n",
    "for dataset, models in metrics.items():\n",
    "    for model, vals in models.items():\n",
    "        rows.append(\n",
    "            {\n",
    "                \"Dataset\": dataset,\n",
    "                \"Modelo\": model,\n",
    "                \"Accuracy\": vals.get(\"accuracy\"),\n",
    "                \"Recall\": vals.get(\"recall\"),\n",
    "                \"F1\": vals.get(\"f1\"),\n",
    "                \"AUC\": vals.get(\"auc\"),\n",
    "            }\n",
    "        )\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19914397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se creo plot de  Accuracy con nombre accuracy_comparison.png\n",
      "Se creo plot de  Recall con nombre recall_comparison.png\n",
      "Se creo plot de  F1 con nombre f1_comparison.png\n",
      "Se creo plot de  AUC con nombre auc_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# graficar métricas comparativas\n",
    "for metric in [\"Accuracy\", \"Recall\", \"F1\", \"AUC\"]:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=df, x=\"Dataset\", y=metric, hue=\"Modelo\")\n",
    "    plt.title(f\"Comparación de {metric} por dataset y modelo\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(title=\"Modelo\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"reports/figures/{metric.lower()}_comparison.png\")\n",
    "    plt.close()\n",
    "    print(f\"Se creo plot de  {metric} con nombre {metric.lower()}_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff43ef27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gráficos guardados en reports/figures/\n"
     ]
    }
   ],
   "source": [
    "print(\"Gráficos guardados en reports/figures/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a8f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_rf': {'model': 'models/uci_rf.joblib', 'X_test': '/data/processed/uci_X_test.csv', 'y_test': '/data/processed/uci_y_test.csv'}, 'heart_failure_rf': {'model': '/models/heart_failure_rf.joblib', 'X_test': '/data/processed/heart_failure_X_test.csv', 'y_test': '/data/processed/heart_failure_y_test.csv'}, 'framingham_logreg': {'model': '/models/framingham_logreg.joblib', 'X_test': '/data/processed/framingham_X_test.csv', 'y_test': '/data/processed/framingham_y_test.csv'}}\n",
      "models/uci_rf.joblib\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/uci_rf.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, paths \u001b[38;5;129;01min\u001b[39;00m model_paths.items():\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(paths[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     model = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     X_test = pd.read_csv(paths[\u001b[33m\"\u001b[39m\u001b[33mX_test\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     27\u001b[39m     y_test = pd.read_csv(paths[\u001b[33m\"\u001b[39m\u001b[33my_test\u001b[39m\u001b[33m\"\u001b[39m]).values.ravel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fede4\\OneDrive\\Documents\\UNRN\\Inteligencia Artificial - 25.2\\cardioPredict\\venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'models/uci_rf.joblib'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modelos a evaluar (ruta a modelo, ruta a X_test, y_test)\n",
    "model_paths = {\n",
    "    \"uci_rf\": {\n",
    "        \"model\": \"../models/uci_rf.joblib\",\n",
    "        \"X_test\": \"../data/processed/uci_X_test.csv\",\n",
    "        \"y_test\": \"../data/processed/uci_y_test.csv\",\n",
    "    },\n",
    "    \"heart_failure_rf\": {\n",
    "        \"model\": \"../models/heart_failure_rf.joblib\",\n",
    "        \"X_test\": \"../data/processed/heart_failure_X_test.csv\",\n",
    "        \"y_test\": \"../data/processed/heart_failure_y_test.csv\",\n",
    "    },\n",
    "    \"framingham_logreg\": {\n",
    "        \"model\": \"../models/framingham_logreg.joblib\",\n",
    "        \"X_test\": \"../data/processed/framingham_X_test.csv\",\n",
    "        \"y_test\": \"../data/processed/framingham_y_test.csv\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(model_paths)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for name, paths in model_paths.items():\n",
    "    print(paths[\"model\"])\n",
    "    model = joblib.load(paths[\"model\"])\n",
    "    X_test = pd.read_csv(paths[\"X_test\"])\n",
    "    y_test = pd.read_csv(paths[\"y_test\"]).values.ravel()\n",
    "\n",
    "    y_score = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "# Línea de azar\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Azar (AUC = 0.5)\")\n",
    "\n",
    "plt.title(\"Curvas ROC por modelo\")\n",
    "plt.xlabel(\"Tasa de falsos positivos (FPR)\")\n",
    "plt.ylabel(\"Tasa de verdaderos positivos (TPR)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reports/figures/roc_curves.png\", dpi=150)\n",
    "plt.close()\n",
    "print(\"✅ Se guardó reports/figures/roc_curves.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
